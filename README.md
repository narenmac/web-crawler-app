Functional Requirements
Start from a list of seed URLs.

Download web pages.

Extract links and metadata.

Recursively crawl discovered URLs (up to a depth or limit).

Store content and metadata in a database.

De-duplicate URLs (avoid revisits).

Handle failures and retries.

Control crawl rate (politeness, robots.txt).

